{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network from Scratch - One step at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import collections\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    '''\n",
    "    Function to load the dataset\n",
    "    '''\n",
    "    \n",
    "    f = h5py.File(path, 'r')\n",
    "    x_key = list(f.keys())[1]\n",
    "    y_key = list(f.keys())[2]\n",
    "    \n",
    "    X_data = f[x_key]\n",
    "    y_data = f[y_key]\n",
    "    \n",
    "    return (X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n",
      "(209,)\n",
      "(50, 64, 64, 3)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_dataset('train_catvnoncat.h5')\n",
    "X_test, y_test = load_dataset('test_catvnoncat.h5')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(12288, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0],-1)).T\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],-1)).T\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 209)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.reshape(y_train, (1,-1))\n",
    "y_test = np.reshape(y_test, (1,-1))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, n_features, n_layers):\n",
    "        self.n_layers = n_layers + 1\n",
    "        self.layer_dims = [1] * (n_layers + 1)\n",
    "        self.activations = [None] * (n_layers + 1)\n",
    "        self.layer_dims[0] = n_features\n",
    "        self.activations[0] = None\n",
    "        self.add_layer_count = 1\n",
    "    '''\n",
    "    Helper functions for calculating the activations\n",
    "    '''\n",
    "    def sigmoid(self, z):\n",
    "        s = np.divide(1, (1 + np.exp(-z)))\n",
    "        return s\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0,z)\n",
    "    \n",
    "    def leaky_relu(self, z):\n",
    "        return np.maximum(0.01*z,z)\n",
    "    \n",
    "    def tanh(self, z):\n",
    "        return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "    \n",
    "    '''\n",
    "    Helper functions to calculate the derivative of the activations\n",
    "    '''\n",
    "    \n",
    "    def sigmoid_back(self, z):\n",
    "        g_prime = np.multiply(self.sigmoid(z), (1 - self.sigmoid(z)))\n",
    "        return g_prime\n",
    "    \n",
    "    def relu_back(self, z):\n",
    "        g_prime = np.where(z <= 0, 0, 1)\n",
    "        return g_prime\n",
    "    \n",
    "    def leaky_relu_back(self, z):\n",
    "        g_prime = np.where(z <= 0, 0.01, 1)\n",
    "        return g_prime\n",
    "    \n",
    "    def tanh_back(z):\n",
    "        g_prime = 1 - (self.tanh(z))**2\n",
    "        return g_prime\n",
    "    \n",
    "    '''\n",
    "    Main Functions\n",
    "    '''\n",
    "    \n",
    "    def forward_prop(self, X, n_layers, layers_dict, training=True):\n",
    "        '''\n",
    "        Performs one forward propagation iteration through entire training set and computes Z and A\n",
    "        Input: Training data X, number of hidden layers + output layer, layers_dict that contains \n",
    "               initial weights and bias, and the activation function for each layer, and a boolean, \n",
    "               training, that denotes training or prediction\n",
    "        Output: Activation at the output layer and cache(Z,A) for each layer stored inside the layers_dict\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        m = X.shape[1]\n",
    "\n",
    "        if not training:\n",
    "            print(\"m: \",m)\n",
    "            temp_A = [None]\n",
    "\n",
    "        for curr_layer in range(1, n_layers):\n",
    "\n",
    "            if curr_layer == 1:\n",
    "                A_prev = X\n",
    "            else:\n",
    "                if training:\n",
    "                    A_prev = layers_dict[str(curr_layer - 1)][\"cache\"][\"A\"]\n",
    "                else:\n",
    "                    A_prev = temp_A[curr_layer-1]\n",
    "\n",
    "            W = layers_dict[str(curr_layer)][\"params\"][\"W\"]\n",
    "            b = layers_dict[str(curr_layer)][\"params\"][\"b\"]\n",
    "\n",
    "            activation = layers_dict[str(curr_layer)][\"activation\"][\"forward\"]\n",
    "\n",
    "            assert(W.shape[1] == A_prev.shape[0])\n",
    "            assert(W.shape[0] == b.shape[0])\n",
    "\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            A = eval(activation)(Z) # Calls the layer's corresponding activation function\n",
    "\n",
    "            assert(A.shape[1] == m)\n",
    "\n",
    "            if training:\n",
    "                '''update the dictionary only if it is training'''\n",
    "                cache = {\n",
    "                    \"A\" : A,\n",
    "                    \"Z\" : Z,\n",
    "                }\n",
    "                layers_dict[str(curr_layer)][\"cache\"] = cache\n",
    "\n",
    "            else:\n",
    "                temp_A.append(A)\n",
    "\n",
    "        return layers_dict, A\n",
    "    \n",
    "    def compute_cost_and_dA(self, y, A):\n",
    "        '''\n",
    "        Method to compute the loss/cost and the first derivative dA\n",
    "        Input: The vector of label of shape (1,m) and Activation of the output layer computed through\n",
    "               forward propagation(also shape (1,m))\n",
    "        Output: Cost J and dA, the derivative of J wrt to output layer\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        m = y.shape[1]\n",
    "\n",
    "        J = ( -1 / m ) * np.sum( np.multiply(y, np.log(A)) + ( np.multiply((1 - y), np.log(1 - A))) )\n",
    "        J = np.squeeze(J)\n",
    "\n",
    "        dA = -( np.divide(y, A) ) + ( np.divide((1 - y), (1 - A)))\n",
    "\n",
    "        assert(dA.shape == A.shape)\n",
    "        assert(J.shape == ())\n",
    "\n",
    "        return J, dA\n",
    "    \n",
    "    def backprop(self, X, dA, layers_dict, n_layers):\n",
    "        '''\n",
    "        Performs single iteration of backpropagation to compute dW and db for each layer\n",
    "        Input: Training data X, dA(first derivative of J computed by compute_cost_and_dA method),\n",
    "               The dictionary containing the parameters, activation, and cache for each layer\n",
    "        Output: Updated version of input dictionary that contains the derivatives dW and db for each\n",
    "                layer\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        m = dA.shape[1]\n",
    "\n",
    "        for curr_layer in range(n_layers-1, 0, -1):\n",
    "\n",
    "            if curr_layer != n_layers - 1:\n",
    "                dA = layers_dict[str(curr_layer + 1)][\"derivative\"][\"dA\"]\n",
    "\n",
    "            if curr_layer == 1:\n",
    "                A_l_minus_one = X\n",
    "            else:\n",
    "                A_l_minus_one = layers_dict[str(curr_layer - 1)][\"cache\"][\"A\"]\n",
    "\n",
    "            Z = layers_dict[str(curr_layer)][\"cache\"][\"Z\"]\n",
    "            W = layers_dict[str(curr_layer)][\"params\"][\"W\"]\n",
    "            activation = layers_dict[str(curr_layer)][\"activation\"][\"backward\"]\n",
    "\n",
    "            #compute dZ[l] = dA[l] * g[l]'(Z[l]), shape: (l,m) * (l,m) = (l,m)\n",
    "            dZ = np.multiply(dA, eval(activation)(Z))\n",
    "\n",
    "            #compute dW[l] = (1/m)(dZ[l]. A[l-1].T), shape: (l,m).(m,l-1) = (l,l-1)\n",
    "            dW = np.multiply((1 / m), np.dot(dZ, A_l_minus_one.T))\n",
    "\n",
    "            #compute db[l] = (1/m)(sum(dZ) across the rows), shape: sum(l,m) = (l,1)\n",
    "            db = np.multiply((1 / m), np.sum(dZ, axis=1, keepdims=True))\n",
    "\n",
    "            #compute dA[l-1] = (W[l].T . dZ[l]), shape: (l-1,l).(l,m) = (l-1,m)\n",
    "            dA = np.dot(W.T, dZ)\n",
    "\n",
    "            derivative = {\n",
    "                \"dW\" : dW,\n",
    "                \"db\" : db,\n",
    "                \"dA\" : dA\n",
    "            }\n",
    "\n",
    "            layers_dict[str(curr_layer)][\"derivative\"] = derivative\n",
    "\n",
    "        return layers_dict\n",
    "    \n",
    "    def initialize_dictionary(self, n_layers, layer_dims, activations):\n",
    "        '''\n",
    "        Method to create a suitable data structure in the form of dictionary and to initialize \n",
    "        weights, bias, and to include the activation functions for each layer\n",
    "        Input: A list containing the number of hidden units for each layer and another list with\n",
    "               corresponding activation functions\n",
    "        Output: A dictionary of dictionaries to store the activation, cache, derivative, params for each \n",
    "                layer. Params will contain W (nhl,nhl-1) and b(nhl,1) for each layer initialized with\n",
    "                random values\n",
    "        '''\n",
    "    \n",
    "        d = collections.defaultdict()\n",
    "\n",
    "        for curr_layer in range(1, n_layers):\n",
    "\n",
    "            if activations:\n",
    "                activation = {\n",
    "                    'forward' : \"self.\" + activations[curr_layer],\n",
    "                    'backward' : \"self.\" + activations[curr_layer] + \"_back\"\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                activation = None\n",
    "\n",
    "            l_minus_one = layer_dims[curr_layer - 1]\n",
    "            l = layer_dims[curr_layer]\n",
    "\n",
    "            W = np.random.randn(l, l_minus_one) * 0.05\n",
    "            b = np.zeros((l, 1))\n",
    "\n",
    "            params = {\n",
    "                \"W\": W, \n",
    "                \"b\": b\n",
    "            }\n",
    "\n",
    "            empty_dict = {\n",
    "                \"activation\" : activation,\n",
    "                \"cache\" : None,\n",
    "                \"derivative\" : None,\n",
    "                \"params\" : params\n",
    "            }\n",
    "\n",
    "            d[str(curr_layer)] = empty_dict\n",
    "\n",
    "        return d\n",
    "    \n",
    "    def update_parameters(self, alpha, n_layers, layers_dict):\n",
    "        '''\n",
    "        Method to update the weights and bias for all the layers\n",
    "        Input: Learning rate alpha, number of hidden layers + output layer, the main dictionary\n",
    "        Output: The input dictionary with updated Weights and bias\n",
    "        '''\n",
    "    \n",
    "        for curr_layer in range(1, n_layers):\n",
    "\n",
    "            W = layers_dict[str(curr_layer)][\"params\"][\"W\"]\n",
    "            b = layers_dict[str(curr_layer)][\"params\"][\"b\"]\n",
    "\n",
    "            dW = layers_dict[str(curr_layer)][\"derivative\"][\"dW\"]\n",
    "            db = layers_dict[str(curr_layer)][\"derivative\"][\"db\"]\n",
    "\n",
    "            assert(W.shape == dW.shape)\n",
    "            assert(b.shape == db.shape)\n",
    "\n",
    "            W = W - np.multiply(alpha, dW)\n",
    "            b = b - np.multiply(alpha, db)\n",
    "\n",
    "            layers_dict[str(curr_layer)][\"params\"][\"W\"] = W\n",
    "            layers_dict[str(curr_layer)][\"params\"][\"b\"] = b\n",
    "\n",
    "        return layers_dict\n",
    "    \n",
    "    def train(self, X_train, y_train, n_layers, layer_dims, activations, epochs, alpha, verbose):\n",
    "        '''\n",
    "        Method to initiate the training. Performs initializing the dictionary, forward propagation,\n",
    "        computes cost, back propagation, and updates the parameters\n",
    "        Input: Data pertaining to the training\n",
    "        Output: Updated dictionary that contains all the required data and list of losses for each epoch\n",
    "        '''\n",
    "\n",
    "        costs = []\n",
    "\n",
    "        layers_dict = self.initialize_dictionary(n_layers=n_layers, layer_dims=layer_dims, activations=activations)\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            layers_dict, A = self.forward_prop(X=X_train, n_layers=n_layers, layers_dict=layers_dict)\n",
    "\n",
    "            J, dA = self.compute_cost_and_dA(y=y_train, A=A)\n",
    "            costs.append(J)\n",
    "\n",
    "\n",
    "            layers_dict = self.backprop(X=X_train, dA=dA, n_layers=n_layers, layers_dict=layers_dict)\n",
    "\n",
    "            layers_dict = self.update_parameters(alpha=alpha, n_layers=n_layers, layers_dict=layers_dict)\n",
    "\n",
    "            if verbose and (epoch == 1 or epoch%100 == 0):\n",
    "\n",
    "                print(\"Epoch: {}, Loss: {}\".format(epoch, J))\n",
    "\n",
    "        return costs, layers_dict\n",
    "    \n",
    "    def get_params(self, n_layers, layers_dict):\n",
    "        '''\n",
    "        Method to cherrypick the parametes from the main dictionary\n",
    "        Input: Main dictionary\n",
    "        Output: params dictionary that has Weights and bias for each layer\n",
    "        '''\n",
    "    \n",
    "        params = {}\n",
    "        for curr_layer in range(1, n_layers):\n",
    "\n",
    "            curr_layer_params = layers_dict[str(curr_layer)][\"params\"]\n",
    "            params[str(curr_layer)] = curr_layer_params\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def predict(self, X, n_layers, layers_dict, threshold):\n",
    "        '''\n",
    "        Method to make predictions\n",
    "        Input: The data for which prediction has to be made, the main dictionary, threshold\n",
    "        Output: A numpy array containing the predictions\n",
    "        '''\n",
    "    \n",
    "        layers_dict, A = self.forward_prop(X=X, n_layers=n_layers, layers_dict=layers_dict, training=False)\n",
    "\n",
    "        predictions = [(lambda pred: 0 if activation < threshold else 1)(activation) for activation in np.squeeze(A)]\n",
    "        predictions = np.array(predictions).reshape(1,-1)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def add_layer(self, hidden_units=2, activation='sigmoid'):\n",
    "        '''\n",
    "        Method to add a new layer to the NN model\n",
    "        Input: number of hidden units and the activation function for the layer\n",
    "        Output: None\n",
    "        '''\n",
    "        \n",
    "        if self.add_layer_count > self.n_layers:\n",
    "            raise IndexError(\"Max layer count reached.\")\n",
    "        self.layer_dims[self.add_layer_count] = hidden_units\n",
    "        if activation not in ['relu', 'leaky_relu', 'sigmoid', 'tanh']:\n",
    "            raise ValueError(\"Unknown activation function: {}\".format(activation))\n",
    "        self.activations[self.add_layer_count] = activation\n",
    "        \n",
    "        print(\"Layer \" + str(self.add_layer_count) + \" added.\")\n",
    "        self.add_layer_count += 1\n",
    "        print(\"Add \" + str(self.n_layers - self.add_layer_count) + \" more layer\")\n",
    "        \n",
    "    \n",
    "    def run(self, X_train, y_train, X_test, y_test, epochs=1000, alpha=0.01, verbose=True, threshold=0.5, save_path=None):\n",
    "        '''\n",
    "        Method that the user can invoke to run training and check the accuracy\n",
    "        Input: Training, test data, path to save the weights, epochs, learning rate, threshold\n",
    "        Output: None\n",
    "        '''\n",
    "        if self.add_layer_count != self.n_layers:\n",
    "            diff = self.n_layers - self.add_layer_count\n",
    "            raise AssertionError(\"Add {} more layer to continue..\".format(diff))\n",
    "    \n",
    "        n_layers = self.n_layers\n",
    "        layer_dims = self.layer_dims\n",
    "        activations = self.activations\n",
    "        \n",
    "        n_features = X_train.shape[0]\n",
    "        m_train = X_train.shape[1]\n",
    "        m_test = X_test.shape[1]\n",
    "\n",
    "        costs, layers_dict = self.train(X_train, y_train, n_layers, layer_dims, activations, epochs, alpha, verbose)\n",
    "\n",
    "        yhat_train = self.predict(X_train, n_layers, layers_dict, threshold)\n",
    "        yhat_test = self.predict(X_test, n_layers, layers_dict, threshold)\n",
    "\n",
    "        train_acc = 100 - np.mean(np.abs(yhat_train - y_train)) * 100\n",
    "        test_acc = 100 - np.mean(np.abs(yhat_test - y_test)) * 100\n",
    "\n",
    "        print(\"Train Accuracy: {:.4f}\".format(train_acc))\n",
    "        print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
    "        \n",
    "        if save_path:\n",
    "            if os.path.exists(save_path):\n",
    "                params = self.get_params(n_layers, layers_dict)\n",
    "                file_name = os.path.join(save_path, str(epochs) + 'ep.txt')\n",
    "                with open(file_name, 'w') as txt_write:\n",
    "                    txt_write.write(str(params))\n",
    "                print(\"Weights saved successfully!\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[0]\n",
    "model = NeuralNetwork(n_features, 5)\n",
    "save_path = \"F://Conda_Scripts//deeplearning.ai//01_Neural_Networks_and_Deep_Learning//03\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 added.\n",
      "Add 4 more layer\n",
      "Layer 2 added.\n",
      "Add 3 more layer\n",
      "Layer 3 added.\n",
      "Add 2 more layer\n",
      "Layer 4 added.\n",
      "Add 1 more layer\n",
      "Layer 5 added.\n",
      "Add 0 more layer\n"
     ]
    }
   ],
   "source": [
    "model.add_layer(hidden_units=64, activation='leaky_relu')\n",
    "model.add_layer(hidden_units=20, activation='leaky_relu')\n",
    "model.add_layer(hidden_units=10, activation='leaky_relu')\n",
    "model.add_layer(hidden_units=4, activation='leaky_relu')\n",
    "model.add_layer(hidden_units=1, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6931329354598631\n",
      "Epoch: 100, Loss: 0.6443976930084547\n",
      "Epoch: 200, Loss: 0.6439657169446765\n",
      "Epoch: 300, Loss: 0.6439557043542569\n",
      "Epoch: 400, Loss: 0.6439492932115576\n",
      "Epoch: 500, Loss: 0.6439418323842193\n",
      "Epoch: 600, Loss: 0.643933050684222\n",
      "Epoch: 700, Loss: 0.6439219784555213\n",
      "Epoch: 800, Loss: 0.6439076791967533\n",
      "Epoch: 900, Loss: 0.6438893999495753\n",
      "Epoch: 1000, Loss: 0.6438638930521184\n",
      "Epoch: 1100, Loss: 0.6438260050644836\n",
      "Epoch: 1200, Loss: 0.6437668851508482\n",
      "Epoch: 1300, Loss: 0.6436641022412896\n",
      "Epoch: 1400, Loss: 0.6434758977725299\n",
      "Epoch: 1500, Loss: 0.6430216197878172\n",
      "Epoch: 1600, Loss: 0.641561342534048\n",
      "Epoch: 1700, Loss: 0.6319023466503358\n",
      "Epoch: 1800, Loss: 0.5482072060250862\n",
      "Epoch: 1900, Loss: 0.6402542721726597\n",
      "Epoch: 2000, Loss: 0.5206987187428366\n",
      "Epoch: 2100, Loss: 0.4666431592350448\n",
      "Epoch: 2200, Loss: 0.382174180174454\n",
      "Epoch: 2300, Loss: 0.32589551087674623\n",
      "Epoch: 2400, Loss: 0.2509222705455521\n",
      "Epoch: 2500, Loss: 0.41140749289783807\n",
      "Epoch: 2600, Loss: 0.28643745911807217\n",
      "Epoch: 2700, Loss: 0.1377051469278484\n",
      "Epoch: 2800, Loss: 0.31341335911924173\n",
      "Epoch: 2900, Loss: 0.6181250608150176\n",
      "Epoch: 3000, Loss: 2.7403250552578915\n",
      "Epoch: 3100, Loss: 0.25834146599550634\n",
      "Epoch: 3200, Loss: 0.3363217470517583\n",
      "Epoch: 3300, Loss: 0.24471149133833517\n",
      "Epoch: 3400, Loss: 0.3247147535861461\n",
      "Epoch: 3500, Loss: 0.23655691193318412\n",
      "Epoch: 3600, Loss: 0.1177015098092526\n",
      "Epoch: 3700, Loss: 0.06707410337267185\n",
      "Epoch: 3800, Loss: 0.5845845839699351\n",
      "Epoch: 3900, Loss: 1.6130159117450713\n",
      "Epoch: 4000, Loss: 0.049409529937297816\n",
      "Epoch: 4100, Loss: 0.017020054370938963\n",
      "Epoch: 4200, Loss: 0.008722548445935134\n",
      "Epoch: 4300, Loss: 0.005483809791552316\n",
      "Epoch: 4400, Loss: 0.0038714083419441624\n",
      "Epoch: 4500, Loss: 0.0029336721789826993\n",
      "Epoch: 4600, Loss: 0.0023499625651200976\n",
      "Epoch: 4700, Loss: 0.001935238194188382\n",
      "Epoch: 4800, Loss: 0.0016420757214080878\n",
      "Epoch: 4900, Loss: 0.0014193333674419604\n",
      "Epoch: 5000, Loss: 0.001249659423780217\n",
      "Epoch: 5100, Loss: 0.0011091469916920127\n",
      "Epoch: 5200, Loss: 0.0009980086913892185\n",
      "Epoch: 5300, Loss: 0.0009027033603169824\n",
      "Epoch: 5400, Loss: 0.0008245058656542048\n",
      "Epoch: 5500, Loss: 0.0007569703849455401\n",
      "Epoch: 5600, Loss: 0.0006990315938598634\n",
      "Epoch: 5700, Loss: 0.0006476274853378747\n",
      "Epoch: 5800, Loss: 0.0006034980365321953\n",
      "Epoch: 5900, Loss: 0.0005631458918433403\n",
      "Epoch: 6000, Loss: 0.0005280021575974358\n",
      "Epoch: 6100, Loss: 0.0004967008524336086\n",
      "Epoch: 6200, Loss: 0.0004680162995810022\n",
      "Epoch: 6300, Loss: 0.0004421012163186843\n",
      "Epoch: 6400, Loss: 0.0004187534714207216\n",
      "Epoch: 6500, Loss: 0.00039751803131594883\n",
      "Epoch: 6600, Loss: 0.0003780514221087589\n",
      "Epoch: 6700, Loss: 0.00036023762368799666\n",
      "Epoch: 6800, Loss: 0.00034418228086299384\n",
      "Epoch: 6900, Loss: 0.00032851790234394746\n",
      "Epoch: 7000, Loss: 0.0003144793474203574\n",
      "Epoch: 7100, Loss: 0.0003014448279084248\n",
      "Epoch: 7200, Loss: 0.0002892871843437133\n",
      "Epoch: 7300, Loss: 0.00027800287757446407\n",
      "Epoch: 7400, Loss: 0.00026762129732316186\n",
      "Epoch: 7500, Loss: 0.0002575533471768055\n",
      "m:  209\n",
      "m:  50\n",
      "Train Accuracy: 100.0000\n",
      "Test Accuracy: 80.0000\n",
      "Weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.run(X_train, y_train, X_test, y_test, epochs=7500, alpha=0.1, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
