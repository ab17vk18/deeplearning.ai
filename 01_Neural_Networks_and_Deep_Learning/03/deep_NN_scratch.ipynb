{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network from Scratch - One step at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import collections\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    '''\n",
    "    Function to load the dataset\n",
    "    '''\n",
    "    \n",
    "    f = h5py.File(path, 'r')\n",
    "    x_key = list(f.keys())[1]\n",
    "    y_key = list(f.keys())[2]\n",
    "    \n",
    "    X_data = f[x_key]\n",
    "    y_data = f[y_key]\n",
    "    \n",
    "    return (X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3)\n",
      "(209,)\n",
      "(50, 64, 64, 3)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_dataset('train_catvnoncat.h5')\n",
    "X_test, y_test = load_dataset('test_catvnoncat.h5')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(12288, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0],-1)).T\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],-1)).T\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 209)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.reshape(y_train, (1,-1))\n",
    "y_test = np.reshape(y_test, (1,-1))\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / X_train.max()\n",
    "X_test = X_test / X_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    s = np.divide(1, (1 + np.exp(-z)))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \n",
    "    return np.maximum(0,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    \n",
    "    return np.maximum(0.01*z,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    \n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(z):\n",
    "    \n",
    "    g_prime = np.multiply(sigmoid(z), (1 - sigmoid(z)))\n",
    "    return g_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_back(z):\n",
    "    \n",
    "    g_prime = np.where(z <= 0, 0, 1)\n",
    "    return g_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_back(z):\n",
    "    \n",
    "    g_prime = np.where(z <= 0, 0.01, 1)\n",
    "    return g_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_back(z):\n",
    "    \n",
    "    g_prime = 1 - (tanh(z))**2\n",
    "    return g_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, n_layers, layers_dict, training=True):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if not training:\n",
    "        print(\"m: \",m)\n",
    "        temp_A = [None]\n",
    "    \n",
    "    for curr_layer in range(1, n_layers):\n",
    "        \n",
    "        if curr_layer == 1:\n",
    "            A_prev = X\n",
    "        else:\n",
    "            if training:\n",
    "                A_prev = layers_dict[str(curr_layer - 1)][\"cache\"][\"A\"]\n",
    "            else:\n",
    "                A_prev = temp_A[curr_layer-1]\n",
    "            \n",
    "        W = layers_dict[str(curr_layer)][\"params\"][\"W\"]\n",
    "        b = layers_dict[str(curr_layer)][\"params\"][\"b\"]\n",
    "        \n",
    "        activation = layers_dict[str(curr_layer)][\"activation\"][\"forward\"]\n",
    "        \n",
    "        assert(W.shape[1] == A_prev.shape[0])\n",
    "        assert(W.shape[0] == b.shape[0])\n",
    "        \n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = eval(activation)(Z) # Calls the layer's corresponding activation function\n",
    "        \n",
    "        #assert(Z.shape == (layer_dims[curr_layer],m))\n",
    "        assert(A.shape[1] == m)\n",
    "        \n",
    "        if training:\n",
    "            \n",
    "            cache = {\n",
    "                \"A\" : A,\n",
    "                \"Z\" : Z,\n",
    "            }\n",
    "        \n",
    "            layers_dict[str(curr_layer)][\"cache\"] = cache\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            temp_A.append(A)\n",
    "\n",
    "                     \n",
    "    return layers_dict, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_and_dA(y, A):\n",
    "    \n",
    "    m = y.shape[1]\n",
    "    \n",
    "    J = ( -1 / m ) * np.sum( np.multiply(y, np.log(A)) + ( np.multiply((1 - y), np.log(1 - A))) )\n",
    "    J = np.squeeze(J)\n",
    "    \n",
    "    dA = -( np.divide(y, A) ) + ( np.divide((1 - y), (1 - A)))\n",
    "    \n",
    "    assert(dA.shape == A.shape)\n",
    "    assert(J.shape == ())\n",
    "    \n",
    "    return J, dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(X, dA, layers_dict, n_layers):\n",
    "    \n",
    "    m = dA.shape[1]\n",
    "    \n",
    "    for curr_layer in range(n_layers-1, 0, -1):\n",
    "        \n",
    "        if curr_layer != n_layers - 1:\n",
    "            dA = layers_dict[str(curr_layer + 1)][\"derivative\"][\"dA\"]\n",
    "            \n",
    "        if curr_layer == 1:\n",
    "            A_l_minus_one = X\n",
    "        else:\n",
    "            A_l_minus_one = layers_dict[str(curr_layer - 1)][\"cache\"][\"A\"]\n",
    "            \n",
    "        Z = layers_dict[str(curr_layer)][\"cache\"][\"Z\"]\n",
    "        W = layers_dict[str(curr_layer)][\"params\"][\"W\"]\n",
    "        activation = layers_dict[str(curr_layer)][\"activation\"][\"backward\"]\n",
    "        \n",
    "        #compute dZ[l] = dA[l] * g[l]'(Z[l]), shape: (l,m) * (l,m) = (l,m)\n",
    "        dZ = np.multiply(dA, eval(activation)(Z))\n",
    "        \n",
    "        #compute dW[l] = (1/m)(dZ[l]. A[l-1].T), shape: (l,m).(m,l-1) = (l,l-1)\n",
    "        dW = np.multiply((1 / m), np.dot(dZ, A_l_minus_one.T))\n",
    "        \n",
    "        #compute db[l] = (1/m)(sum(dZ) across the rows), shape: sum(l,m) = (l,1)\n",
    "        db = np.multiply((1 / m), np.sum(dZ, axis=1, keepdims=True))\n",
    "        \n",
    "        #compute dA[l-1] = (W[l].T . dZ[l]), shape: (l-1,l).(l,m) = (l-1,m)\n",
    "        dA = np.dot(W.T, dZ)\n",
    "        \n",
    "        derivative = {\n",
    "            \"dW\" : dW,\n",
    "            \"db\" : db,\n",
    "            \"dA\" : dA\n",
    "        }\n",
    "        \n",
    "        layers_dict[str(curr_layer)][\"derivative\"] = derivative\n",
    "        \n",
    "    return layers_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dictionary(n_layers, layer_dims, activations=None):\n",
    "    \n",
    "    d = collections.defaultdict()\n",
    "    \n",
    "    if activations is not None:\n",
    "        assert(len(activations) == n_layers)\n",
    "    \n",
    "    for curr_layer in range(1, n_layers):\n",
    "        \n",
    "        if activations:\n",
    "            activation = {\n",
    "                'forward' : activations[curr_layer],\n",
    "                'backward' : activations[curr_layer] + \"_back\"\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            activation = None\n",
    "        \n",
    "        l_minus_one = layer_dims[curr_layer - 1]\n",
    "        l = layer_dims[curr_layer]\n",
    "        \n",
    "        W = np.random.randn(l, l_minus_one) * 0.05\n",
    "        b = np.zeros((l, 1))\n",
    "        \n",
    "        params = {\n",
    "            \"W\": W, \n",
    "            \"b\": b\n",
    "        }\n",
    "        \n",
    "        empty_dict = {\n",
    "            \"activation\" : activation,\n",
    "            \"cache\" : None,\n",
    "            \"derivative\" : None,\n",
    "            \"params\" : params\n",
    "        }\n",
    "        \n",
    "        d[str(curr_layer)] = empty_dict\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(alpha, n_layers, layers_dict):\n",
    "    \n",
    "    for curr_layer in range(1, n_layers):\n",
    "        \n",
    "        W = layers_dict[str(curr_layer)][\"params\"][\"W\"]\n",
    "        b = layers_dict[str(curr_layer)][\"params\"][\"b\"]\n",
    "        \n",
    "        dW = layers_dict[str(curr_layer)][\"derivative\"][\"dW\"]\n",
    "        db = layers_dict[str(curr_layer)][\"derivative\"][\"db\"]\n",
    "        \n",
    "        assert(W.shape == dW.shape)\n",
    "        assert(b.shape == db.shape)\n",
    "        \n",
    "        W = W - np.multiply(alpha, dW)\n",
    "        b = b - np.multiply(alpha, db)\n",
    "        \n",
    "        layers_dict[str(curr_layer)][\"params\"][\"W\"] = W\n",
    "        layers_dict[str(curr_layer)][\"params\"][\"b\"] = b\n",
    "        \n",
    "    return layers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, n_layers, layer_dims, activations, epochs, alpha, verbose):\n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    layers_dict = initialize_dictionary(n_layers=n_layers, layer_dims=layer_dims, activations=activations)\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        layers_dict, A = forward_prop(X=X_train, n_layers=n_layers, layers_dict=layers_dict)\n",
    "        \n",
    "        J, dA = compute_cost_and_dA(y=y_train, A=A)\n",
    "        costs.append(J)\n",
    "        \n",
    "        \n",
    "        layers_dict = backprop(X=X_train, dA=dA, n_layers=n_layers, layers_dict=layers_dict)\n",
    "        \n",
    "        layers_dict = update_parameters(alpha=alpha, n_layers=n_layers, layers_dict=layers_dict)\n",
    "        \n",
    "        if verbose and (epoch == 1 or epoch%100 == 0):\n",
    "            \n",
    "            print(\"Epoch: {}, Loss: {}\".format(epoch, J))\n",
    "            \n",
    "    return costs, layers_dict\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(n_layers, layers_dict):\n",
    "    \n",
    "    params = {}\n",
    "    \n",
    "    for curr_layer in range(1, n_layers):\n",
    "        \n",
    "        curr_layer_params = layers_dict[str(curr_layer)][\"params\"]\n",
    "        params[str(curr_layer)] = curr_layer_params\n",
    "        \n",
    "    return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, n_layers, layers_dict, threshold):\n",
    "    \n",
    "    layers_dict, A = forward_prop(X=X, n_layers=n_layers, layers_dict=layers_dict, training=False)\n",
    "    \n",
    "    predictions = [(lambda pred: 0 if activation < threshold else 1)(activation) for activation in np.squeeze(A)]\n",
    "    predictions = np.array(predictions).reshape(1,-1)\n",
    "    \n",
    "    return predictions    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(X_train, y_train, X_test, y_test, layer_dims, activations, epochs=1000, alpha=0.01, verbose=True, threshold=0.5):\n",
    "    \n",
    "    n_layers = len(layer_dims)\n",
    "    n_activations = len(activations)\n",
    "    n_features = X_train.shape[0]\n",
    "    m_train = X_train.shape[1]\n",
    "    m_test = X_test.shape[1]\n",
    "    \n",
    "    assert (layer_dims[0] == n_features), \"First entry in the layer_dims should be number of training features\"\n",
    "    assert (activations[0] == None), \"Input layer's activation should be None\"\n",
    "    assert (n_layers == n_activations), \"Number of layers and their corresponding activations do not match\"\n",
    "    \n",
    "    costs, layers_dict = train(X_train, y_train, n_layers, layer_dims, activations, epochs, alpha, verbose)\n",
    "    \n",
    "    yhat_train = predict(X_train, n_layers, layers_dict, threshold)\n",
    "    yhat_test = predict(X_test, n_layers, layers_dict, threshold)\n",
    "    \n",
    "    train_acc = 100 - np.mean(np.abs(yhat_train - y_train)) * 100\n",
    "    test_acc = 100 - np.mean(np.abs(yhat_test - y_test)) * 100\n",
    "    \n",
    "    print(\"Train Accuracy: {:.4f}\".format(train_acc))\n",
    "    print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
    "    \n",
    "    params = get_params(n_layers, layers_dict)\n",
    "      \n",
    "    return params   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[0]\n",
    "layer_dims = [64, 16, 1]\n",
    "layer_dims = [n_features] + layer_dims\n",
    "activations = ['leaky_relu', 'leaky_relu', 'sigmoid']\n",
    "activations = [None] + activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12288, 64, 16, 1]\n",
      "[None, 'leaky_relu', 'leaky_relu', 'sigmoid']\n"
     ]
    }
   ],
   "source": [
    "print(layer_dims)\n",
    "print(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.70722184617536\n",
      "Epoch: 100, Loss: 0.6165365084081325\n",
      "Epoch: 200, Loss: 0.5180424459363124\n",
      "Epoch: 300, Loss: 0.39547744931858264\n",
      "Epoch: 400, Loss: 0.3056790565160951\n",
      "Epoch: 500, Loss: 0.12313121990322329\n",
      "Epoch: 600, Loss: 0.08374881487605544\n",
      "Epoch: 700, Loss: 0.04040256478579695\n",
      "Epoch: 800, Loss: 0.011683974312896757\n",
      "Epoch: 900, Loss: 0.006491258995309498\n",
      "Epoch: 1000, Loss: 0.004275134375809059\n",
      "Epoch: 1100, Loss: 0.0030899423148545123\n",
      "Epoch: 1200, Loss: 0.002375321431583761\n",
      "Epoch: 1300, Loss: 0.0019032742490780678\n",
      "Epoch: 1400, Loss: 0.0015731721259739705\n",
      "Epoch: 1500, Loss: 0.0013328924869407592\n",
      "m:  209\n",
      "m:  50\n",
      "Train Accuracy: 100.0000\n",
      "Test Accuracy: 80.0000\n"
     ]
    }
   ],
   "source": [
    "params = run(X_train, y_train, X_test, y_test, layer_dims, activations, epochs=1500, alpha=0.04, verbose=True, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
