{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify! \n",
    "\n",
    "Have you ever wanted to make your text messages more expressive? Your emojifier app will help you do that. \n",
    "So rather than writing:\n",
    ">\"Congratulations on the promotion! Let's get coffee and talk. Love you!\"   \n",
    "\n",
    "The emojifier can automatically turn this into:\n",
    ">\"Congratulations on the promotion! üëç Let's get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\"\n",
    "\n",
    "* You will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('emojify_data.csv')\n",
    "X_test, Y_test = read_csv('tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French macaroon is so tasty üç¥\n",
      "work is horrible üòû\n",
      "I am upset üòû\n",
      "throw the ball ‚öæ\n",
      "Good joke üòÑ\n",
      "what is your favorite baseball game ‚öæ\n",
      "I cooked meat üç¥\n",
      "stop messing around üòû\n",
      "I want chinese food üç¥\n",
      "Let us go play baseball ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'I am frustrated' has label index 3, which is emoji üòû\n",
      "Label index 3 in one-hot encoding format is [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 20\n",
    "print(\"Sentence '{}' has label index {}, which is emoji {}\".format(X_train[idx],Y_train[idx],label_to_emoji(Y_train[idx])))\n",
    "print(\"Label index {} in one-hot encoding format is {}\".format(Y_train[idx],Y_oh_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 400000 words in the vocabulary\n",
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "idx = 289846\n",
    "print(\"There are a total of \" + str(len(word_to_index.keys())) + \" words in the vocabulary\")\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojify - Basic\n",
    "\n",
    "Sentence -> list of words -> word vectors -> compute their average -> feed in a NN with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and averages its value into a single vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    \n",
    "    Returns:\n",
    "    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split sentence into list of lower case words\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    # Average the word vectors by looping over the words in the sentence\n",
    "    total = 0\n",
    "    for w in words:\n",
    "        total += word_to_vec_map[w]\n",
    "    avg = total / len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n",
      "It's shape =  (50,)\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)\n",
    "print(\"It's shape = \", avg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \"\"\"\n",
    "    Model to train word vector representations in numpy.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, numpy array of sentences as strings, of shape (m, 1)\n",
    "    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    learning_rate -- learning_rate for the stochastic gradient descent algorithm\n",
    "    num_iterations -- number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    pred -- vector of predictions, numpy-array of shape (m, 1)\n",
    "    W -- weight matrix of the softmax layer, of shape (n_y, n_h)\n",
    "    b -- bias of the softmax layer, of shape (n_y,)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    m = Y.shape[0]\n",
    "    n_y = 6\n",
    "    n_h = 50\n",
    "    \n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros(shape=(n_y, ))\n",
    "    \n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        for i in range(m):\n",
    "            \n",
    "            # Forward prop\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "            \n",
    "            # Compute Loss\n",
    "            cost = - np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
    "            \n",
    "            # Backprop\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape((n_y, 1)), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            # Update the parameters\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183,)\n",
      "(183,)\n",
      "(183, 5)\n",
      "French macaroon is so tasty\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "(183, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.2364712890128793\n",
      "Accuracy: 0.34972677595628415\n",
      "Epoch: 100 --- cost = 0.6553092778527012\n",
      "Accuracy: 0.8579234972677595\n",
      "Epoch: 200 --- cost = 0.49857195184550807\n",
      "Accuracy: 0.9016393442622951\n",
      "Epoch: 300 --- cost = 0.4107257294060037\n",
      "Accuracy: 0.9180327868852459\n",
      "[[4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9289617486338798\n",
      "Test set:\n",
      "Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "\n",
      "i adore you üòÑ\n",
      "i love you ‚ù§Ô∏è\n",
      "funny lol üòÑ\n",
      "lets play with a ball ‚öæ\n",
      "food is ready üç¥\n",
      "not feeling happy üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56,)\n",
      "           ‚ù§Ô∏è    ‚öæ    üòÑ    üòû   üç¥\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            6    0    1    0    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            4    0   14    0    0   18\n",
      "3            0    0    1   15    0   16\n",
      "4            0    0    0    1    6    7\n",
      "All         10    8   16   16    6   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojify - LSTM Model\n",
    "\n",
    "Sentences -> List of words -> List of Indices -> Embeddings -> Embedding Matrix -> Embedding Layer ->\n",
    "LSTM -> Dropout -> LSTM -> Dropout -> Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Initialize X_indices as numpy zeros matrix of shape(no_of_examples, max_len_of_sentences)\n",
    "    X_indices = np.zeros(shape=(m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words\n",
    "        sentence_words = X[i].lower().split()\n",
    "                \n",
    "        # Loop over the words of sentence_words\n",
    "        for j, word in enumerate(sentence_words):\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[word]\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"the\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Step 1\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape(no_of_word_in_corpus, len_of_GloVe_vector)\n",
    "    emb_matrix = np.zeros(shape=(vocab_len, emb_dim))\n",
    "    \n",
    "    # Step 2\n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Step 3\n",
    "    # Define Keras non-trainable embedding layer\n",
    "    embedding_layer = keras.layers.embeddings.Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n",
    "    \n",
    "    # Step 4\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -0.3403\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the Input layer\n",
    "    sentence_indices = keras.layers.Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden units. \n",
    "    # return_sequences is set to True in all LSTM layers except the last LSTM layer\n",
    "    X = keras.layers.LSTM(units=128, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = keras.layers.Dropout(rate=0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden units\n",
    "    X = keras.layers.LSTM(units=128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = keras.layers.Dropout(rate=0.5)(X)\n",
    "    # Propagate X through a Dense layer with 5 output units(n_classes) and softmax activation\n",
    "    X = keras.layers.Dense(units=5)(X)\n",
    "    # Softmax Activation layer\n",
    "    X = keras.layers.Activation(activation='softmax')(X)\n",
    "     \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = keras.models.Model(inputs=sentence_indices, outputs=X)\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_32 (Embedding)     (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a model instance\n",
    "\n",
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in correct format\n",
    "\n",
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "183/183 [==============================] - 11s 58ms/step - loss: 1.5595 - acc: 0.3497\n",
      "Epoch 2/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 1.4342 - acc: 0.3661\n",
      "Epoch 3/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 1.3161 - acc: 0.4754\n",
      "Epoch 4/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 1.1314 - acc: 0.5301\n",
      "Epoch 5/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.9550 - acc: 0.6175\n",
      "Epoch 6/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.9035 - acc: 0.6667\n",
      "Epoch 7/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.8317 - acc: 0.7158\n",
      "Epoch 8/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.6686 - acc: 0.7486\n",
      "Epoch 9/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.6160 - acc: 0.7432\n",
      "Epoch 10/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.6831 - acc: 0.7268\n",
      "Epoch 11/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.5351 - acc: 0.8087\n",
      "Epoch 12/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.4103 - acc: 0.8415\n",
      "Epoch 13/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.5682 - acc: 0.7923\n",
      "Epoch 14/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.5634 - acc: 0.7814\n",
      "Epoch 15/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.4286 - acc: 0.8361\n",
      "Epoch 16/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4379 - acc: 0.8361\n",
      "Epoch 17/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.4068 - acc: 0.8798\n",
      "Epoch 18/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4771 - acc: 0.8361\n",
      "Epoch 19/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.2710 - acc: 0.8962\n",
      "Epoch 20/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.2777 - acc: 0.9016\n",
      "Epoch 21/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.3802 - acc: 0.8415\n",
      "Epoch 22/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4982 - acc: 0.7869\n",
      "Epoch 23/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.2886 - acc: 0.8962\n",
      "Epoch 24/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.2218 - acc: 0.9126\n",
      "Epoch 25/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.4060 - acc: 0.8415\n",
      "Epoch 26/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1685 - acc: 0.9454\n",
      "Epoch 27/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1950 - acc: 0.9344\n",
      "Epoch 28/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.2826 - acc: 0.8907\n",
      "Epoch 29/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.2691 - acc: 0.9180\n",
      "Epoch 30/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.3192 - acc: 0.8907\n",
      "Epoch 31/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1575 - acc: 0.9454\n",
      "Epoch 32/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.2127 - acc: 0.9399\n",
      "Epoch 33/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1599 - acc: 0.9344\n",
      "Epoch 34/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1524 - acc: 0.9508\n",
      "Epoch 35/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.2504 - acc: 0.8907\n",
      "Epoch 36/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.1777 - acc: 0.9235\n",
      "Epoch 37/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0904 - acc: 0.9727\n",
      "Epoch 38/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.2392 - acc: 0.9071\n",
      "Epoch 39/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0846 - acc: 0.9727\n",
      "Epoch 40/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.1104 - acc: 0.9508\n",
      "Epoch 41/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0393 - acc: 0.9945\n",
      "Epoch 42/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0435 - acc: 0.9836A: 0s - loss: 0.0532 - acc: 0.\n",
      "Epoch 43/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.3785 - acc: 0.8907\n",
      "Epoch 44/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0978 - acc: 0.9727\n",
      "Epoch 45/50\n",
      "183/183 [==============================] - 1s 4ms/step - loss: 0.0477 - acc: 0.9836\n",
      "Epoch 46/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.1288 - acc: 0.9727\n",
      "Epoch 47/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0655 - acc: 0.9836\n",
      "Epoch 48/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0204 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.1243 - acc: 0.9563\n",
      "Epoch 50/50\n",
      "183/183 [==============================] - 1s 3ms/step - loss: 0.0530 - acc: 0.9836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1687a589be0>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 3s 52ms/step\n",
      "\n",
      "Test accuracy =  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòÑ prediction: he got a very nice raise\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: she got me a nice present\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: he is a good friend\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: We had such a lovely dinner tonight\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: valentine day is near\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n",
      "Expected emoji:‚ù§Ô∏è prediction: I like your jacket \tüòÑ\n",
      "Expected emoji:üç¥ prediction: I did not have breakfast üòû\n"
     ]
    }
   ],
   "source": [
    "# This code allows you to see the mislabelled examples\n",
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
